<head>
  <link href="main.css" rel="stylesheet"></link>
</head>
<body>
  <div class="content">
    <form id="search-form">
      <input type="search" placeholder="search"></input>
      <div class="query-error"></div>
      <div class="controls">
        <button type="submit">Search</button>
        <button type="reset">Reset</button>
      </div>
    </form>
    <ol>
      
        <li data-bio-id="1993.sigirconf_conference-93.31">
          <article>
            <header>
              <h2 data-field="name">On the Evaluation of Boolean Operators in the Extended Boolean Retrieval Framework</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTThe retrieval models based on the extended boolean retrieval framework, e.g., the fuzzy set model and the extended boolean model have been proposed in the past to provide the conventional boolean retrievat system with the document ranking facility.However, due to undesirable properties of evaluation formulas for the AND and OR operations, the former generates incorrect ranked output in certain cases and the latter suffers from the complexity of computation. There have been a variety of fuzzy operators to replace the evaluation formulas. In this paper we first investigate the behavioral aspects of the fuzzy operators and address important issues to affect retrieval effectiveness. We then defiie an operator class called positively compensatory operators giving high retrieval effectiveness, and present a pair of positively compensatory operators providing high retrieval efficiency as well as high retrieval effectiveness. All the claims are justifkxt through experiments.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1985.sigirconf_conference-85.8">
          <article>
            <header>
              <h2 data-field="name">Automatic Assignment of Soft Boolean Operators</h2>
            </header>

            <section data-field="body">
              <p> AbstractThe conventional bibliographic retrieval systems are based on Boolean query formulations and inverted file implementations. Such systems provide rapid responses in answer to search queries but they are not easy to use by uninitiated patrons. An extended Boolean retrieval strategy has been devised in which the Boolean operators are treated more or less strictly, depending on the setting of a special parameter, known as the pvalue. The extended system is much more forgiving than the conventional system, and provides better retrieval effectiveness. In this study various problems associated with the determination of appropriate p-values are discussed, and suggestions are made for an automatic assignment of p-values. Evaluation output is included to illustrate the operations of the suggested procedures.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1986.sigirconf_conference-86.21">
          <article>
            <header>
              <h2 data-field="name">On Extending the Vector Space Model for Boolean Query Processing</h2>
            </header>

            <section data-field="body">
              <p> Abstract. An infamation retrieval model, named the Generaliied Vectm Spice Model (GVSM). is extended m handle situations where queries are specitied as (extended) Boolean expressions. It is shown tbat this unified model, unlike currently available alternatives, has the advantage of inwrpating tetm cortelations inm the retrieval process. 'Ilte query language extension is attractive in the sense that most of the aIgebraic properties of tbe strict Boolean language are still preserved. Although the experimental results for extended Boolean retrieval are not always better than the vector processing method, the developments here am signiecant in facilitating commercially available retrieval systems to benefit from the vector based methods. The proposed scheme is compared m the pnorm model advanced by Salmn snd coworkers. An important conclusion is that it is desirable m investigate further extensions that can offer the benefits of both proposals.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2000.sigirconf_conference-2000.42">
          <article>
            <header>
              <h2 data-field="name">Latent semantic indexing model for boolean query formulation</h2>
            </header>

            <section data-field="body">
              <p> AbstractA new model named Boolean Latent Semantic Indexing model based on the Singular Value Decomposition and Boolean query formulation is introduced. While the Singular Value Decomposition alleviates the problems of lexical matching in the traditional information retrieval model, Boolean query formulation can help users to make precise representation of their information search needs. Retrieval experiments on a number of test collections seem to show that the proposed model achieves substantial performance gains over the Latent Semantic Indexing model.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2004.ntcir_workshop-2004.80">
          <article>
            <header>
              <h2 data-field="name">Study on the Combination of Probabilistic and Boolean IR Models for WWW Documents Retrieval</h2>
            </header>

            <section data-field="body">
              <p> In this paper, we describe our information retrieval (IR) system that is used for the NTCIR-4 Web Task A. First, we introduce our IR system, which is based on the probabilistic IR model. This system is quite similar to the Okapi system, and uses both a word index and a phrase index comprising combinations of two adjacent words. Second, we propose a method for clarifying queries that combines the probabilistic IR model and the Boolean IR model. Since it is not easy to construct a Boolean query that covers all relevant documents, a mechanism for clarifying the Boolean query is required. In this paper, we propose "appropriate Boolean query reformulation for IR" (ABRIR) that support Boolean query formation and score documents based on combining probabilistic and Boolean IR models. Finally, we discuss the effectiveness of the method based on the results of experiments.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1997.sigirconf_conference-97.15">
          <article>
            <header>
              <h2 data-field="name">Computationally Tractable Probabilistic Modeling of Boolean Operators</h2>
            </header>

            <section data-field="body">
              <p> The inference network model of information retrieval allows for a probabilistic interpretation of Boolean query operators. Prior work has shown, however, that these operators do not perform as well as the pnorrn operators developed in the context of the vector space model. The design of alternativeoperators in the inference network framework must contend with the issue of computational tractability. We define a flexible class of link matrices that are natural candidates for the implementation of Boolean operators and art 0(n2) algorithm for the computation of probabilities involving link matrices of this class. We present experimental results indicating that Boolean operators implemented in terms of link mahices from this class perform as well as pnorm operators.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2011.sigirconf_conference-2011.85">
          <article>
            <header>
              <h2 data-field="name">Automatic boolean query suggestion for professional search</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTIn professional search environments, such as patent search or legal search, search tasks have unique characteristics: 1) users interactively issue several queries for a topic, and 2) users are willing to examine many retrieval results, i.e., there is typically an emphasis on recall. Recent surveys have also verified that professional searchers continue to have a strong preference for Boolean queries because they provide a record of what documents were searched. To support this type of professional search, we propose a novel Boolean query suggestion technique. Specifically, we generate Boolean queries by exploiting decision trees learned from pseudo-labeled documents and rank the suggested queries using query quality predictors. We evaluate our algorithm in simulated patent and medical search environments. Compared with a recent effective query generation system, we demonstrate that our technique is effective and general.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2006.ipm_journal-ir0anthology0volumeA42A2.0">
          <article>
            <header>
              <h2 data-field="name">Adaptive relevance feedback method of extended Boolean model using hierarchical clustering techniques</h2>
            </header>

            <section data-field="body">
              <p> AbstractThe relevance feedback process uses information obtained from a user about a set of initially retrieved documents to improve subsequent search formulations and retrieval performance. In extended Boolean models, the relevance feedback implies not only that new query terms must be identified and re-weighted, but also that the terms must be connected with Boolean And/Or operators properly. Salton et al. proposed a relevance feedback method, called DNF (disjunctive normal form) method, for a well established extended Boolean model. However, this method mainly focuses on generating Boolean queries but does not concern about re-weighting query terms. Also, this method has some problems in generating reformulated Boolean queries. In this study, we investigate the problems of the DNF method and propose a relevance feedback method using hierarchical clustering techniques to solve those problems. We also propose a neural network model in which the term weights used in extended Boolean queries can be adjusted by the usersÕ relevance feedbacks.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1981.sigirconf_conference-81.8">
          <article>
            <header>
              <h2 data-field="name">Performance Measurement in a Fuzzy Retrieval Environment</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTWe shall consider retrieval performance measures for generalized (non-Boolean) queries and indexing functions.The meanings of recall and precision in such a generalized system will be discussed.Finally, we shall explore the meaning and difficulty of using such measures to compare Boolean and non-Boolean retrieval systems.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2009.trec_conference-2009.21">
          <article>
            <header>
              <h2 data-field="name">Experiments with the Negotiated Boolean Queries of the TREC 2009 Legal Track</h2>
            </header>

            <section data-field="body">
              <p> For our participation in the Batch Task of the TREC 2009 Legal Track, we produced several retrieval sets to compare experimental Boolean, vector, fusion and relevance feedback techniques for e-Discovery requests. In this paper, we have reported not just the mean scores of the experimental approaches but also the largest per-topic impacts of the techniques for several measures. The experimental automatic relevance feedback technique was found to attain a statistically significant gain over the reference Boolean result in both the mean Precision@B and F 1 @K measures. 1 Open Text eDOCS SearchServer and Open Text eDOCS Suite are trademarks or registered trademarks of Open Text Corporation in the United States of America, Canada, the European Union and/or other countries. This list of trademarks is not exhaustive. Other trademarks, registered trademarks, product names, company names, brands and service names mentioned herein are property of Open Text Corporation or other respective owners.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.trec_conference-2008.15">
          <article>
            <header>
              <h2 data-field="name">Experiments with the Negotiated Boolean Queries of the TREC 2008 Legal Track</h2>
            </header>

            <section data-field="body">
              <p> We analyze the results of several experimental runs submitted for the TREC 2008 Legal Track. In the Ad Hoc task, we found that rank-based merging of vector results with the reference Boolean results produced a statistically significant increase in mean F 1 @K and Recall@B compared to just using the reference Boolean results. In the Relevance Feedback task, we found that the investigated relevance feedback technique, when merged with the reference Boolean results, produced some substantial increases in Recall@B r without any substantial decreases on individual topics.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1990.sigirconf_conference-90.28">
          <article>
            <header>
              <h2 data-field="name">Extended Boolean Retrieval: A Heuristic Approach?</h2>
            </header>

            <section data-field="body">
              <p>Extended Boolean Retrieval: A Heuristic Approach?</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2000.tois_journal-ir0anthology0volumeA18A1.1">
          <article>
            <header>
              <h2 data-field="name">Shortest-substring retrieval and ranking</h2>
            </header>

            <section data-field="body">
              <p> We present a model for arbitrary passage retrieval using Boolean queries. The model is applied to the task of ranking documents, or other structural elements, in the order of their expected relevance. Features such as phrase matching, truncation, and stemming integrate naturally into the model. Properties of Boolean algebra are obeyed, and the exact-match semantics of Boolean retrieval are preserved. Simple inverted-list file structures provide an efficient implementation. Retrieval effectiveness is comparable to that of standard ranking techniques. Since global statistics are not used, the method is of particular value in distributed environments. Since ranking is based on arbitrary passages, the structural elements to be ranked may be specified at query time and do not need to be restricted to predefined elements.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2009.ipm_journal-ir0anthology0volumeA45A2.10">
          <article>
            <header>
              <h2 data-field="name">On the use of negation in Boolean IR queries</h2>
            </header>

            <section data-field="body">
              <p> a b s t r a c tThe negation operator, in various forms in which it appears in Information Retrieval queries, is investigated. The applications include negated terms in Boolean queries, more specifically in the presence of metrical constraints, but also negated characters used in the definition of extended keywords by means of regular expressions. Exact definitions are suggested and their usefulness is shown on several examples. Finally, some implementation issues are discussed, in particular as to the order in which the terms of long queries, with or without negated keywords, should be processed, and efficient heuristics for choosing a good order are suggested.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1999.tois_journal-ir0anthology0volumeA17A1.0">
          <article>
            <header>
              <h2 data-field="name">Predicate Rewriting for Translating Boolean Queries in a Heterogeneous Information System</h2>
            </header>

            <section data-field="body">
              <p> Searching over heterogeneous information sources is difficult in part because of the nonuniform query languages. Our approach is to allow users to compose Boolean queries in one rich front-end language. For each user query and target source, we transform the user query into a subsuming query that can be supported by the source but that may return extra documents. The results are then processed by a filter query to yield the correct final results. In this article we introduce the architecture and associated mechanism for query translation. In particular, we discuss techniques for rewriting predicates in Boolean queries into native subsuming forms, which is a basis of translating complex queries. In addition, we present experimental results for evaluating the cost of postfiltering. We also discuss the drawbacks of this approach and cases when it may not be effective. We have implemented prototype versions of these mechanisms and demonstrated them on heterogeneous Boolean systems.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1989.sigirconf_conference-89.5">
          <article>
            <header>
              <h2 data-field="name">Design of a Browsing Interface for Information Retrieval</h2>
            </header>

            <section data-field="body">
              <p> AbstractIn conventional Boolean retrieval systems,</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2007.trec_conference-2007.22">
          <article>
            <header>
              <h2 data-field="name">Experiments with the Negotiated Boolean Queries of the TREC 2007 Legal Discovery Track</h2>
            </header>

            <section data-field="body">
              <p> We analyze the results of several experimental runs submitted for the TREC 2007 Legal Track (also sometimes known as the Legal Discovery Track). We submitted 4 boolean query runs (the initial proposal by the defendant, the rejoinder by the plaintiff, the final negotiated query, and a variation of the final query which had proximity distances doubled). We submitted 2 vector query runs (one based on the keywords of the final negotiated query, and another based on the (natural language) request text). We submitted a blind feedback run based on the final negotiated boolean query. Finally, we submitted a fusion run of the final boolean, request text and final vector runs. We found that none of the runs had a higher mean estimated Recall@B than the original final negotiated boolean query. 1 Livelink, Open Text TM and SearchServer TM are trademarks or registered trademarks of Open Text Corporation in the United States of America, Canada, the European Union and/or other countries. This list of trademarks is not exhaustive. Other trademarks, registered trademarks, product names, company names, brands and service names mentioned herein are property of Open Text Corporation or other respective owners.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2003.clef_workshop-2003.32">
          <article>
            <header>
              <h2 data-field="name">Selective Compound Splitting of Swedish Queries for Boolean Combinations of Truncated Terms</h2>
            </header>

            <section data-field="body">
              <p> Abstract. In languages that use compound words such as Swedish, it is often neccessary to split compound words when indexing documents or queries. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken here is to expand a query with the leading constituents of the compound words. Every query term is truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. This approach increases recall in a rather uncontrolled way, so we use a Boolean quorum-level search method to rank documents both according to a tf-idf factor but also to the number of matching Boolean combinations.The Boolean combinations performed relatively well, taking into consideration that the queries were very short (maximum of five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2010.cikm_conference-2010.75">
          <article>
            <header>
              <h2 data-field="name">Result-size estimation for information-retrieval subqueries</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTEstimating the approximate result size of a query before its execution based on small summary statistics is important for query optimization in database systems and for other facets of query processing. This also holds for queries over text databases. Research on selectivity estimation for such queries has focused on Boolean retrieval, i.e., a document may be relevant for the query or not. But with the coalescence of database and information retrieval (IR) technology, selectivity estimation for other, more sophisticated relevance functions is gaining importance as well. These models generate a query-specific distribution of the documents over the [0, 1]-interval. With document distributions, selectivity estimation means estimating how many documents are how similar to a given query. The problem is much more complex than selectivity estimation in the Boolean context: Beside document frequency, query results also depend on other characteristics such as term frequencies and document lengths. Selectivity estimation must take them into account as well. This paper proposes and evaluates a technique for estimating the result of retrieval queries with non-Boolean relevance functions. It estimates discretized document distributions over the range of the relevance function. Despite the complexity, compared to Boolean selectivity estimation, it requires little additional data, and the additional data can be stored in existing data structures with little extensions. Our evaluation demonstrates the effectiveness of our technique.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2003.clef_workshop-2003w.44">
          <article>
            <header>
              <h2 data-field="name">Selective Compound Splitting of Swedish Queries for Boolean Combinations of Truncated Terms</h2>
            </header>

            <section data-field="body">
              <p> Swedish is a compounding language, and therefore it is important to split compound words so that useful word constituents can be found. One of the problems is that it is difficult to find constituents that express a concept similar to that expressed by the compound. The approach taken in this paper is to look at how the leading constituent of the compound word can be used to expand a search query. The constituent was added to the original query, while still keeping the compound. Every word was then truncated so as to increase recall by hopefully finding other compounds with the leading constituent as prefix. Since this approach increase recall in a rather uncontrolled way, we also used a Boolean quorum-level type of query combination so that documents were ranked according to both the tf-idf factor but also to the number of matching Boolean combinations. The Boolean combinations performed relatively well, taken into consideration that the queries were very short (maximum five search terms). Also included in this paper are the results of two other methods we are currently working on in our lab; one for re-ranking search results on the basis of stylistic analysis of documents, and one for dimensionality reduction using Random Indexing.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.sigirconf_conference-2008.57">
          <article>
            <header>
              <h2 data-field="name">TF-IDF uncovered: a study of theories and probabilities</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTInterpretations of TF-IDF are based on binary independence retrieval, Poisson, information theory, and language modelling. This paper contributes a review of existing interpretations, and then, TF-IDF is systematically related to the probabilities P (q|d) and P (d|q). Two approaches are explored: a space of independent, and a space of disjoint terms. For independent terms, an "extreme" query/non-query term assumption uncovers TF-IDF, and an analogy of P (d|q) and the probabilistic odds O(r|d, q) mirrors relevance feedback. For disjoint terms, a relationship between probability theory and TF-IDF is established through the integral R 1 x dx = log x. This study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document-query independence (DQI) measure, and interestingly, an integral of the DQI over the term occurrence probability leads to TF-IDF.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2019.clef_conference-2019w.167">
          <article>
            <header>
              <h2 data-field="name">Twitter Feeds Profiling with TF-IDF</h2>
            </header>

            <section data-field="body">
              <p> Paper describes our approach in celebrity profiling task at CLEF 2019 conference. Our method is based on TF-IDF feature extraction method combined with random forest classifier. We were mainly focused on preprocessing phase, where we implemented multiple methods for a text normalization such as emoji transformation, lemmatization, URL replacing. The biggest problem was class imbalance, which we tried to resolve by using synthetic oversampling techniques. 2 Task Description Task is to profile given celebrity Twitter feed. Our task is to predict four traitsauthor's occupation, birthyear, fame and gender. An average F1 macro score amongst all traits was chosen by organizers as a final evaluation score. Classes in training dataset are heavily imbalanced, especially nonbinary gender class. Since, birthyear prediction is extremely difficult, score of birthyear trait is calculated leniently.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2019.clef_conference-2019w.58">
          <article>
            <header>
              <h2 data-field="name">Celebrity Profiling using TF-IDF, Logistic Regression, and SVM</h2>
            </header>

            <section data-field="body">
              <p> This paper aims to describe a TF-IDF approach based on word bigrams and n-grams at a character level used in the Celebrity Profiling competition at PAN CLEF 2019. 3 Preprocessing The applied preprocessing involved the following techniques:</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2016.ecir_workshop-2016bir.8">
          <article>
            <header>
              <h2 data-field="name">Bag of Works Retrieval: TF*IDF Weighting of Co-cited Works</h2>
            </header>

            <section data-field="body">
              <p> Although it is not presently possible in any system, the style of retrieval described here combines familiar components-co-citation linkages of documents and TF*IDF weighting of terms-in a novel way that could be implemented in citation-enhanced digital libraries of the future. Rather than entering keywords, the user enters a string identifying a work, called a seed, to retrieve the strings identifying other works that are co-cited with the seed. Each of the latter is part of a "bag of works," and it presumably has both a co-citation count with the seed and an overall citation count in the database. These two counts can be plugged into a standard formula for TF*IDF weighting such that all the co-cited items can be ranked for relevance to the seed. The result is analogous to, but different from, traditional "bag of words" retrieval. Certain properties of the ranking are illustrated with the top and bottom items co-cited with a classic paper by Marcia J. Bates, "The design of browsing and berrypicking techniques for the online search interface." However, the properties apply to bag of works retrievals in general and have implications for users (e.g., humanities scholars, domain analysts) that go beyond any one example.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.sigirconf_conference-2013.36">
          <article>
            <header>
              <h2 data-field="name">A novel TF-IDF weighting scheme for effective ranking</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTTerm weighting schemes are central to the study of information retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query.Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.cikm_workshop-2008pikm.15">
          <article>
            <header>
              <h2 data-field="name">Clustering the topics using TF-IDF for model fusion</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTUsers tend to express their queries in various ways: sometimes they use more general terms, sometimes more specific terms. Information retrieval systems need to be able to accommodate this variety of user needs. Some retrieval models perform better when the queries are general, others perform better when the queries are more specific, and others when a combination is available. In this paper we are looking for a system that will perform well in all these cases, we present a new method for combining the results of different models in order to improve the performance on a difficult task: Information Retrieval from spontaneous speech. Our technique is based on clustering the training topics according to their tf-idf (term frequency-inverse document frequency) properties, and selecting the best models for each cluster. When the system runs on a test topic, the cluster of the topic needs to be determined and the combination of models of this cluster is used. We report improvements on the Malach collection used at CLEF-CLSR 2007.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2018.sigirconf_conference-2018.210">
          <article>
            <header>
              <h2 data-field="name">Merchandise Recommendation for Retail Events with Word Embedding Weighted Tf-idf and Dynamic Query Expansion</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTTo recommend relevant merchandises for seasonal retail events, we rely on item retrieval from marketplace inventory. With feedback to expand query scope, we discuss keyword expansion candidate selection using word embedding similarity, and an enhanced tf-idf formula for expanded words in search ranking.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2018.wwwconf_conference-2018c.320">
          <article>
            <header>
              <h2 data-field="name">Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTProper formulation of features plays an important role in shorttext classification tasks as the amount of text available is very little. In literature, Term Frequency -Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2017.clef_conference-2017w.149">
          <article>
            <header>
              <h2 data-field="name">Using TF-IDF n-gram and Word Embedding Cluster Ensembles for Author Profiling</h2>
            </header>

            <section data-field="body">
              <p> This paper presents our approach and results for the 2017 PAN Author Profiling Shared Task. Language-specific corpora were provided for four langauges: Spanish, English, Portuguese, and Arabic. Each corpus consisted of tweets authored by a number of Twitter users labeled with their gender and the specific variant of their language which was used in the documents (e.g. Brazilian or European Portuguese). The task was to develop a system to infer the same attributes for unseen Twitter users. Our system employs an ensemble of two probabilistic classifiers: a Logistic regression classifier trained on TF-IDF transformed n-grams and a Gaussian Process classifier trained on word embedding clusters derived for an additional, external corpus of tweets.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2017.clef_conference-2017w.145">
          <article>
            <header>
              <h2 data-field="name">UniNE at CLEF 2017: TF-IDF and Deep-Learning for Author Profiling</h2>
            </header>

            <section data-field="body">
              <p> This paper describes and evaluates a strategy for author profiling using TF-IDF and a Deep-Learning model based on Convolutional Neural Networks. We applied this strategy to the author profiling task of the PAN17 challenge and show that it can be applied to different languages (English, Spanish, Portuguese and Arabic). As features, we suggest using a simple cleaning method for both models, and for the Deep-Learning model, a matrix of 2-grams of letters with punctuation marks, beginning and ending 2-grams, as features. Applying this strategy, we determine that the TFIDF-based model is the best one for language variety classification and that the Deep-Learning model achieve the highest accuracy on gender classification. The evaluations are based on four tweet collections (PAN AUTHOR PROFILING task at CLEF 2017).</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2014.wwwconf_conference-2014c.92">
          <article>
            <header>
              <h2 data-field="name">Bing-SF-IDF+: semantics-driven news recommendation</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTContent-based news recommendation is traditionally performed using the cosine similarity and TF-IDF weighting scheme for terms occurring in news messages and user profiles. Semantics-driven variants such as SF-IDF additionally take into account term meaning by exploiting synsets from semantic lexicons. However, they ignore the various semantic relationships between synsets, providing only for a limited understanding of news semantics. Moreover, semanticsbased weighting techniques are not able to handle -often crucial -named entities, which are usually not present in semantic lexicons. Hence, we extend SF-IDF by also considering the synset semantic relationships, and by employing named entity similarities using Bing page counts. Our proposed method, Bing-SF-IDF+, outperforms TF-IDF and SF-IDF in terms of F1 scores and kappa statistics.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.cikm_conference-2008.28">
          <article>
            <header>
              <h2 data-field="name">BNS feature scaling: an improved representation over tf-idf for svm text classification</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTIn the realm of machine learning for text classification, TF·IDF is the most widely used representation for real-valued feature vectors. However, IDF is oblivious to the training class labels and naturally scales some features inappropriately. We replace IDF with Bi-Normal Separation (BNS), which has been previously found to be excellent at ranking words for feature selection filtering. Empirical evaluation on a benchmark of 237 binary text classification tasks shows substantially better accuracy and F-measure for a Support Vector Machine (SVM) by using BNS scaling. A wide variety of other feature representations were later tested and found inferior, as well as binary features with no scaling. Moreover, BNS scaling yielded better performance without feature selection, obviating the need for feature selection.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2014.ntcir_conference-2014.14">
          <article>
            <header>
              <h2 data-field="name">Combining TF-IDF Text Retrieval with an Inverted Index over Symbol Pairs in Math Expressions: The Tangent Math Search Engine at NTCIR 2014</h2>
            </header>

            <section data-field="body">
              <p> We report on the system design and NTCIR-Math-2 task results for the Tangent math-aware search engine. Tangent uses a federated search over two indices: 1) a TF-IDF textual search engine (Lucene), and 2) a query-by-expression engine. Query-by-expression is performed using a bag-ofwords approach where expressions are represented by pairs of symbols computed from symbol layout trees (e.g. as expressed in L A T E X or Presentation MathML). Extensions to support matrices and prefix subscripts and superscripts are described. Our system produced the highest highly + partially relevant Precision@5 result for the main text/math query task (92%), and the highest Top-1 specific-item recall for the Wikipedia query-by-expression subtask (68%). The current implementation is slow and produces large indices for large corpora, but we believe this can be ameliorated. Source code for our system is publicly available.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.sigirconf_conference-2013.120">
          <article>
            <header>
              <h2 data-field="name">Composition of TF normalizations: new insights on scoring functions for ad hoc IR</h2>
            </header>

            <section data-field="body">
              <p>MOTIVATIONFang et al. introduced in [3] a set of heuristic retrieval constraints that any scoring function used for ad hoc inforPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. mation retrieval (IR) should satisfy. In particular, these constraints involve term frequency, term discrimination, document length and the interactions between them. For instance, they stated that a scoring function should favor document matching more distinct query terms. It is one of the earliest works that formally defined the properties that both the TF and the IDF components of any weighting model should possess. It is a unifying theory in IR that applies to the vector space model (TF-IDF  The definition of these constraints contributed to the improvement of the overall effectiveness of most modern scoring functions. Constraints on the term frequency result in successive normalizations on the raw TF, each one satisfying one or more properties. In our work, we intended to go one step further and we propose the use of composition to explain how the normalizations are applied successively in the general TF×IDF weighting scheme. In section 2, we describe in details the mathematical framework we designed. In section 3, we present the experiments we conducted over standard datasets and the results obtained that indicate how important the order of composition is, along with a novel and effective weighting model, namely TF l•δ•p ×IDF. Finally, in section 4, we conclude and mention future work.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2019.clef_conference-2019w.166">
          <article>
            <header>
              <h2 data-field="name">Twitter Bots and Gender Detection using Tf-idf</h2>
            </header>

            <section data-field="body">
              <p> As the amount of unstructured data increases, value (and the number) of models that can infer information from this data also increases. This paper presents another such model that can perform bots and gender detection on Twitter using just the tweets from the respective Twitter user. We show that a simple frequency based approach with a machine learning algorithm i.e., SVM can achieve high accuracy if the preprocessing is done right. In English language. our model detects bots with an accuracy of 91% and gender with an accuracy of 82%. Main strength of this model is its simplicity along-with the ease with which it can be used with other languages.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.cikm_conference-2013.10">
          <article>
            <header>
              <h2 data-field="name">Graph-of-word and TW-IDF: new approach to ad hoc IR</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTIn this paper, we introduce novel document representation (graph-of-word) and retrieval model (TW-IDF) for ad hoc IR. Questioning the term independence assumption behind the traditional bag-of-word model, we propose a different representation of a document that captures the relationships between the terms using an unweighted directed graph of terms. From this graph, we extract at indexing time meaningful term weights (TW) that replace traditional term frequencies (TF) and from which we define a novel scoring function, namely TW-IDF, by analogy with TF-IDF. This approach leads to a retrieval model that consistently and significantly outperforms BM25 and in some cases its extension BM25+ on various standard TREC datasets. In particular, experiments show that counting the number of different contexts in which a term occurs inside a document is more effective and relevant to search than considering an overall concave term frequency in the context of ad hoc IR.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2015.airs_conference-2015.33">
          <article>
            <header>
              <h2 data-field="name">Beyond tf-idf and Cosine Distance in Documents Dissimilarity Measure</h2>
            </header>

            <section data-field="body">
              <p> Abstract. In vector space model, different types of term weighting schemes are used to adjust bag-of-words document vectors in order to improve the performance of the most widely used cosine distance. Even though the cosine distance with some term weighting schemes result in more reliable (dis)similarity measure in some data sets, it may not perform well in others because of the underlying assumptions of the term weighting schemes. In this paper, we argue that the explicit adjustment of bag-of-words document vectors using term weighting is not required if a data-dependent dissimilarity measure called mp-dissimilarity is used. Our empirical result in document retrieval task reveals that mp with the simplest binary bag-of-words representation is either better or competitive to the cosine distance with the best performing state-of-the-art term weighting scheme in four widely used benchmark document collections.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2014.clef_conference-2014w.92">
          <article>
            <header>
              <h2 data-field="name">Using Noun Phrases and Tf-idf for Plagiarized Document Retrieval</h2>
            </header>

            <section data-field="body">
              <p> This paper describes an approach submitted to the 2014 PAN competition for the source retrieval sub-task [7]. Both independent term and phrasal queries are generated, using either term frequency-inverse document frequency or noun phrases to select the terms.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2005.spire_conference-2005.33">
          <article>
            <header>
              <h2 data-field="name">Deriving TF-IDF as a Fisher Kernel</h2>
            </header>

            <section data-field="body">
              <p>Deriving TF-IDF as a Fisher Kernel</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2002.ntcir_workshop-2002.55">
          <article>
            <header>
              <h2 data-field="name">Sentence Extraction by tf/idf and Position Weighting from Newspaper Articles</h2>
            </header>

            <section data-field="body">
              <p> Recently, lots of researchers are focusing their interests on the development of summarization systems from large volume sources combined with knowledge acquisition techniques such as information extraction, text mining or information retrieval. Some of these techniques are implemented according to the speci c knowledge in the domain or the genre from the source document. In this paper, we will discuss Japanese Newspaper Domain Knowledge in order to make a summary. My system is implemented with the sentence extraction approach and weighting strategy to mine from a number of documents.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1985.sigirconf_conference-85.4">
          <article>
            <header>
              <h2 data-field="name">Generalized Vector Space Model in Information Retrieval</h2>
            </header>

            <section data-field="body">
              <p> Abstract. In information retrieval, it is common to model index terms and documents as vectore in a suitably defined vector space. The main di]ficulty with this approach is that the explicit repreeentation of term vectors is not known a priorL For th~ mason, the vector space model adopted by Salton for the SMART system treats the terms as a set of orthogonal vectom In such a model it is often necessary to adopt a separate, corrective procedure to take into account the correlations between terms. In this paper, we propose a systematic method (the generalized vector space model) to compute term correlations directly from automatic indexing scheme. We also demonstrate how such correlations can be included with minimal modification in the existing vector based information retrieval systems. The preliminary experimental . results obtained from the new model are very encouraging.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2000.sigirconf_conference-2000.57">
          <article>
            <header>
              <h2 data-field="name">Collaborative filtering and the generalized vector space model</h2>
            </header>

            <section data-field="body">
              <p> AbstractCollaborative filtering is a technique for recommending documents to users based on how similar their tastes are to other users. If two users tend to agree on what they like, the system will recommend the same documents to them. The generalized vector space model of information retrieval represents a document by a vector of its similarities to all other documents. The process of collaborative filtering is nearly identical to the process of retrieval using GVSM in a matrix of user ratings. Using this observation, a model for filtering collaboratively using document content is possible.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2004.ntcir_workshop-2004.61">
          <article>
            <header>
              <h2 data-field="name">Patent Map Generation Using Concept-Based Vector Space Model</h2>
            </header>

            <section data-field="body">
              <p> This paper proposes a patent map generation system using concept-based vector space model and presents evaluation results from the NTCIR-4 patent feasibility study (FS) task. The concept-base is a knowledge base of words, which expresses each word as an associated vector. The word vectors are computed based on word co-occurrence in a target document set. Therefore, the word vectors reflect target documents' characteristics. Each document in the target document set is expressed as a vector that is composed of vectors associated with words included in the document. The word vectors and document vectors are positioned in an identical vector space and the relevant degree of similarity between any two words and/or documents can be computed as a cosine coefficient of the two vectors. Taking advantage of this model, problems sections and solutions sections of patent documents are expressed as vectors, then, they are clustered and the label word for each cluster is chosen from words which give high cosine coefficient to the center of gravity of the cluster. A trial of generating patent maps for NTCIR-4 patent FS task topics using the system has been done. Comparing with humangenerated patent maps, the system provides fairly good accuracy of clustering of target patents but poor accuracy of cluster labeling.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1992.sigirconf_conference-92.14">
          <article>
            <header>
              <h2 data-field="name">An Analysis of Vector Space Models Based on Computational Geometry</h2>
            </header>

            <section data-field="body">
              <p> AbstractThis paper analyzes the properties, structures and limitations of vector-based models for information retrieval from the computational geometry point of view. It is shown that both the pseudo-cosine and the standard vector space models can be viewed as special cases of a generalized linear model. More importantly, both the necessary and sufficient conditions have been identified, under which ranking functions such as the inner-product, cosine, pseudo-cosine, Dice, covariance and productmoment correlation measures can be used to rank the documents. The structure of the solution region for acceptable ranking is analyzed and an algorithm for vectors is suggested.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.cikm_conference-2012.154">
          <article>
            <header>
              <h2 data-field="name">Adapting vector space model to ranking-based collaborative filtering</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTCollaborative filtering (CF) is an effective technique addressing the information overload problem. Recently ranking-based CF methods have shown advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we seek accuracy improvement of ranking-based CF through adaptation of the vector space model, where we consider each user as a document and her pairwise relative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Then we use cosine similarity to select a neighborhood of users for the target user to make recommendations. Experiments on benchmarks in comparison with the state-of-the-art methods demonstrate the promise of our approach.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2004.ntcir_workshop-2004.70">
          <article>
            <header>
              <h2 data-field="name">Question Answering System Using Concept-Based Vector Space Model</h2>
            </header>

            <section data-field="body">
              <p> This paper presents the architecture of the Concept-based Vector Space Mod el Question Answering System (CBVSM-QAS) develop ed at the Nagaoka University of Technology (NUT) and used in the 4-th NTCIR workshop Question Answering Challenge (QAC) evaluation. The CBVSM-QAS runs on the factual question, which corresponds to the subtask-1 in the NTCIR-4's QAC. One major concept of this system is the idea of placing the whole data set in the concept-based vector space, and searching of the answer for each question is done by calculating the nearest newspaper's document vector. In this paper, the architecture of the system used during the formal run of NTCIR-4's QAC and the architecture of the system after some improvement is introduced.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2011.ipm_journal-ir0anthology0volumeA47A3.6">
          <article>
            <header>
              <h2 data-field="name">A relational vector space model using an advanced weighting scheme for image retrieval</h2>
            </header>

            <section data-field="body">
              <p> a b s t r a c tIn this paper, we lay out a relational approach for indexing and retrieving photographs from a collection. The increase of digital image acquisition devices, combined with the growth of the World Wide Web, requires the development of information retrieval (IR) models and systems that provide fast access to images searched by users in databases. The aim of our work is to develop an IR model suited to images, integrating rich semantics for representing this visual data and user queries, which can also be applied to large corpora.Our proposal merges the vector space model of IR -widely tested in textual IR -with the conceptual graph (CG) formalism, based on the use of star graphs (i.e. elementary CGs made up of a single relation connected to some concepts representing image objects). A novel weighting scheme for star graphs, based on image objects size, position, and image heterogeneity is outlined. We show that integrating relations into the vector space model through star graphs increases the system's precision, and that the results are comparable to those from graph projection systems, and also that they shorten processing time for user queries.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2011.ipm_journal-ir0anthology0volumeA47A3.0">
          <article>
            <header>
              <h2 data-field="name">An IPC-based vector space model for patent retrieval</h2>
            </header>

            <section data-field="body">
              <p> a b s t r a c tDetermining requirements when searching for and retrieving relevant information suited to a user's needs has become increasingly important and difficult, partly due to the explosive growth of electronic documents. The vector space model (VSM) is a popular method in retrieval procedures. However, the weakness in traditional VSM is that the indexing vocabulary changes whenever changes occur in the document set, or the indexing vocabulary selection algorithms, or parameters of the algorithms, or if wording evolution occurs. The major objective of this research is to design a method to solve the afore-mentioned problems for patent retrieval. The proposed method utilizes the special characteristics of the patent documents, the International Patent Classification (IPC) codes, to generate the indexing vocabulary for presenting all the patent documents. The advantage of the generated indexing vocabulary is that it remains unchanged, even if the document sets, selection algorithms, and parameters are changed, or if wording evolution occurs. Comparison of the proposed method with two traditional methods (entropy and chi-square) in manual and automatic evaluations is presented to verify the feasibility and validity. The results also indicate that the IPC-based indexing vocabulary selection method achieves a higher accuracy and is more satisfactory.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2018.ipm_journal-ir0anthology0volumeA54A1.5">
          <article>
            <header>
              <h2 data-field="name">Beyond vector space model for hierarchical Arabic text classification: A Markov chain approach</h2>
            </header>

            <section data-field="body">
              <p> A B S T R A C TThe vector space model (VSM) is a textual representation method that is widely used in documents classification. However, it remains to be a space-challenging problem. One attempt to alleviate the space problem is by using dimensionality reduction techniques, however, such techniques have deficiencies such as losing some important information. In this paper, we propose a novel text classification method that neither uses VSM nor dimensionality reduction techniques. The proposed method is a space efficient method that utilizes the first order Markov model for hierarchical Arabic text classification. For each category and sub-category, a Markov chain model is prepared based on the neighboring characters sequences. The prepared models are then used for scoring documents for classification purposes. For evaluation, we used a hierarchical Arabic text data collection that contains 11,191 documents that belong to eight topics distributed into 3-levels. The experimental results show that the Markov chains based method significantly outperforms the baseline system that employs the latent semantic indexing (LSI) method. That is, the proposed method enhances the F1-measure by 3.47%. The novelty of this work lies on the idea of decomposing words into sequences of characters, which found to be a promising approach in terms of space and accuracy. Based on our best knowledge, this is the first attempt to conduct research for hierarchical Arabic text classification with such relatively large data collection.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2002.trec_conference-2002.16">
          <article>
            <header>
              <h2 data-field="name">Coupling Named Entity Recognition, Vector-Space Model and Knowledge Bases for TREC 11 Question Answering Track</h2>
            </header>

            <section data-field="body">
              <p> In this paper, we present a question-answering system combining Named Entity Recognition, Vector-Space Model and Knowledge Bases to validate answers candidates. Applying this hybrid approach, for our first participation in the TREC Q&A.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2018.tois_journal-ir0anthology0volumeA36A4.3">
          <article>
            <header>
              <h2 data-field="name">Neural Vector Spaces for Unsupervised Information Retrieval</h2>
            </header>

            <section data-field="body">
              <p> We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed. NVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2014.ntcir_conference-2014.27">
          <article>
            <header>
              <h2 data-field="name">HULTECH at the NTCIR-11 IMine Task: Mining Intents with Continuous Vector Space Models</h2>
            </header>

            <section data-field="body">
              <p> In this paper, we present our participation in the Subtopic Mining subtask of the NTCIR-11 IMine task, for the English language. Our participation presents a novel strategy for intent mining given a list of candidates for a specific query topic. This strategy is based on a topic exploration through the use of continuous vector space models for each of the candidates based on classical vectorial operations. Our best run outperforms the other participants' submissions in terms of F-score and achieves a high position in the general ranking.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2007.trec_conference-2007.55">
          <article>
            <header>
              <h2 data-field="name">Passage Retrieval with Vector Space and Query-Level Aspect Models</h2>
            </header>

            <section data-field="body">
              <p> This report describes the joint work by Kyoto University and the University of Melbourne for the TREC Genomics Track in 2007. As with 2006, the task for this year was the retrieval of passages from a biomedical document collection. The overall framework of our system from 2006 remains unchanged and is comprised of two parts: a paragraph-level retrieval system and a passage extraction system. These two systems are based on the vector space model and a probabilistic word-based aspect model, respectively. This year, we have adopted numerous changes to our 2007 system which we believe corrected some problems.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2005.ecir_conference-2005.8">
          <article>
            <header>
              <h2 data-field="name">Encoding XML in Vector Spaces</h2>
            </header>

            <section data-field="body">
              <p>Main Contributions and Guided Tour(1)We develop a framework for vector space XML indexing (Section 2) through the notion of tree filters. The choice of these filters governs the index size as well as its retrieval effectiveness. We measure index sizes for a class of tree filters derived from paths in a document (Section 2.2). (2) We benchmark our indexes on INEX Content Only queries (Section 3) 4 . We thus show that structure encoded in the vector space helps retrieval quality (Section 3.5), but at the expense of significantly larger indexes (Section 2.2). (3) We introduce randomized indexes (Section 2.2). Vector space encodings for XML are challenging in the absence of reliable DTD's. Whereas previous work handled this through additional calculations outside the vector space, randomized indexing lets us preserve the vector space framework. (4) We apply our framework to the classification and clustering of XML documents (Section 4) using standard vector space algorithms.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2006.trec_conference-2006.14">
          <article>
            <header>
              <h2 data-field="name">Combining Vector-Space and Word-Based Aspect Models for Passage Retrieval</h2>
            </header>

            <section data-field="body">
              <p> This report summarizes the work done at Kyoto University and the University of Melbourne for the TREC 2006 Genomics Track. The single task for this year was to retrieve passages from a biomedical document collection. We devised a system made of two parts to deal with this problem. The first part was an existing IR system based on the vector-space model. The second part was a newly developed probabilistic word-based aspect model for identifying passages within relevant documents (or paragraphs).</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1986.sigirconf_conference-86.21">
          <article>
            <header>
              <h2 data-field="name">On Extending the Vector Space Model for Boolean Query Processing</h2>
            </header>

            <section data-field="body">
              <p> Abstract. An infamation retrieval model, named the Generaliied Vectm Spice Model (GVSM). is extended m handle situations where queries are specitied as (extended) Boolean expressions. It is shown tbat this unified model, unlike currently available alternatives, has the advantage of inwrpating tetm cortelations inm the retrieval process. 'Ilte query language extension is attractive in the sense that most of the aIgebraic properties of tbe strict Boolean language are still preserved. Although the experimental results for extended Boolean retrieval are not always better than the vector processing method, the developments here am signiecant in facilitating commercially available retrieval systems to benefit from the vector based methods. The proposed scheme is compared m the pnorm model advanced by Salmn snd coworkers. An important conclusion is that it is desirable m investigate further extensions that can offer the benefits of both proposals.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2016.fire_workshop-2016ws.4">
          <article>
            <header>
              <h2 data-field="name">From Vector Space Models to Vector Space Models of Semantics</h2>
            </header>

            <section data-field="body">
              <p>From Vector Space Models to Vector Space Models of Semantics</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2006.sigirconf_conference-2006.10">
          <article>
            <header>
              <h2 data-field="name">Music structure based vector space retrieval</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTThis paper proposes a novel framework for music content indexing and retrieval. The music structure information, i.e., timing, harmony and music region content, is represented by the layers of the music structure pyramid. We begin by extracting this layered structure information. We analyze the rhythm of the music and then segment the signal proportional to the inter-beat intervals. Thus, the timing information is incorporated in the segmentation process, which we call Beat Space Segmentation. To describe Harmony Events, we propose a two-layer hierarchical approach to model the music chords. We also model the progression of instrumental and vocal content as Acoustic Events. After information extraction, we propose a vector space modeling approach which uses these events as the indexing terms. In queryby-example music retrieval, a query is represented by a vector of the statistics of the n-gram events. We then propose two effective retrieval models, a hard-indexing scheme and a soft-indexing scheme. Experiments show that the vector space modeling is effective in representing the layered music information, achieving 82.5% top-5 retrieval accuracy using 15-sec music clips as the queries. The soft-indexing outperforms hard-indexing in general.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.spire_conference-2012.36">
          <article>
            <header>
              <h2 data-field="name">Relevance Feedback Method Based on Vector Space Basis Change</h2>
            </header>

            <section data-field="body">
              <p> Abstract. The idea of relevance feedback (RF) is to involve the user in the retrieval process to improve the final result set by reformulating the query. The most commonly used methods in RF aim to rewrite the user query. In the vector space model, RF is usually undertaken by reweighting the query terms without any modification in the vector space basis. In this paper we propose a RF method based on vector space basis change without any modification on the query term weights. The aim of our method is to find a basis which gives a better representation of the documents such that the relevant documents are gathered and the irrelevant ones are kept away from the relevant documents.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2016.fire_conference-2016w.30">
          <article>
            <header>
              <h2 data-field="name">CEN@Amrita: Information Retrieval on CodeMixed Hindi-English Tweets Using Vector Space Models</h2>
            </header>

            <section data-field="body">
              <p> One of the major challenges nowadays is Information retrieval from social media platforms. Most of the information on these platforms is informal and noisy in nature. It makes the Information retrieval task more challenging. The task is even more difficult for twitter because of its character limitation per tweet. This limitation bounds the user to express himself in condensed set of words. In the context of India, scenario is little more complicated as users prefer to type in their mother tongue but lack of input tools force them to use Roman script with English embeddings. This combination of multiple languages written in the Roman script makes the Information retrieval task even harder. Query processing for such CodeMixed content is a difficult task because query can be in either of the language and it need to be matched with the documents written in any of the language. In this work, we dealt with this problem using Vector Space Models which gave significantly better results than the other participants. The Mean Average Precision (MAP) for our system was 0.0315 which was second best performance for the subtask.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1997.sigirconf_conference-97.26">
          <article>
            <header>
              <h2 data-field="name">Overlapping Statistical Word Indexing: A New Indexing Method for Japanese Text</h2>
            </header>

            <section data-field="body">
              <p> AktractBecause word boundaries are not apparently indicated in Asiart languages including Japanese, word indexing cannot simply be applied. Although dictionary-based text segmentation techniques enable word indexing, they have some problems such as dictionary maintenance. N-gram indexing, another conventional indexing method, suffers from increase in index size. This paper proposes a new statistical indexing method. We fist propose a segmentation method for Japanese text which uses statistical information of characters. It needs only a small amount of statistic information and computation, and does not need constant maintenance. We secondly propose a new indexing strategy which extracts some overlapping segments in addition to the segments extracted using the existing strategy. Thus it increases the effectiveness of retrieval.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1997.sigirjournals_journal-ir0anthology0volumeA31A1.5">
          <article>
            <header>
              <h2 data-field="name">A Blueprint for Automatic Indexing</h2>
            </header>

            <section data-field="body">
              <p> Abstract:This note summarizes some of the currently available insights in automatic indexing. The emphasis is on aspects that are expected to be useful in practical automatic indexing applications. The discussion is necessarily cursory, but the references will lead interested readers to a deeper treatment of the indexing problem.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1999.sigirconf_conference-99.8">
          <article>
            <header>
              <h2 data-field="name">Probabilistic Latent Semantic Indexing</h2>
            </header>

            <section data-field="body">
              <p> AbstractProbabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain speci c synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing LSI by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and de nes a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching metho d s a s w ell as over LSI. In particular, the combination of models with di erent dimensionalities has proven to be advantageous.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1986.sigirconf_conference-86.31">
          <article>
            <header>
              <h2 data-field="name">Two Models of Retrieval with Probabilistic Indexing</h2>
            </header>

            <section data-field="body">
              <p> AbstractWe describe two retrieval modeb for probabilistic indexing. The binary independence indexing (BII)  model is a generalised version of the Maron & Kuhne indexing model. In thin model, the indexing weight of a deecriptor in a document is an e&mate of the probability of relevance of thii document with mpect to queries wing thin deacrip tor. The retrieval-with-probabilistic-indexing @PI) model ia suited to different kinds of probabilistic indexing. Therefore we aneume that each indexing model has its own concept of 'correctnew? to which the probabilities relate. The concept of correctness is qot necessarily identical with the concept of relevance, it ia only required to depend on relevance. In addition to the probabilistic indexing weights, the RPI model provides the posaibiity of relevance weighting of search terms. Both retrieval modele are compared in experiments, showing equally good results.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.ipm_journal-ir0anthology0volumeA48A5.5">
          <article>
            <header>
              <h2 data-field="name">MapReduce indexing strategies: Studying scalability and efficiency</h2>
            </header>

            <section data-field="body">
              <p> a b s t r a c tIn Information Retrieval (IR), the efficient indexing of terabyte-scale and larger corpora is still a difficult problem. MapReduce has been proposed as a framework for distributing data-intensive operations across multiple processing machines. In this work, we provide a detailed analysis of four MapReduce indexing strategies of varying complexity. Moreover, we evaluate these indexing strategies by implementing them in an existing IR framework, and performing experiments using the Hadoop MapReduce implementation, in combination with several large standard TREC test corpora. In particular, we examine the efficiency of the indexing strategies, and for the most efficient strategy, we examine how it scales with respect to corpus size, and processing power. Our results attest to both the importance of minimising data transfer between machines for IO intensive tasks like indexing, and the suitability of the per-posting list MapReduce indexing strategy, in particular for indexing at a terabyte-scale. Hence, we conclude that MapReduce is a suitable framework for the deployment of large-scale indexing.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2009.sigirconf_workshop-2009lsdsir.3">
          <article>
            <header>
              <h2 data-field="name">Comparing Distributed Indexing: To MapReduce or Not?</h2>
            </header>

            <section data-field="body">
              <p> Information Retrieval (IR) systems require input corpora to be indexed. The advent of terabyte-scale Web corpora has reinvigorated the need for efficient indexing. In this work, we investigate distributed indexing paradigms, in particular within the auspices of the MapReduce programming framework. In particular, we describe two indexing approaches based on the original MapReduce paper, and compare these with a standard distributed IR system, the MapReduce indexing strategy used by the Nutch IR platform, and a more advanced MapReduce indexing implementation that we propose. Experiments using the Hadoop MapReduce implementation and a large standard TREC corpus show our proposed MapReduce indexing implementation to be more efficient than those proposed in the original paper.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2004.sigirconf_conference-2004.14">
          <article>
            <header>
              <h2 data-field="name">Locality preserving indexing for document representation</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTDocument representation and indexing is a key problem for document analysis and processing, such as clustering, classification and retrieval. Conventionally, Latent Semantic Indexing (LSI) is considered effective in deriving such an indexing. LSI essentially detects the most representative features for document representation rather than the most discriminative features. Therefore, LSI might not be optimal in discriminating documents with different semantics. In this paper, a novel algorithm called Locality Preserving Indexing (LPI) is proposed for document indexing. Each document is represented by a vector with low dimensionality. In contrast to LSI which discovers the global structure of the document space, LPI discovers the local structure and obtains a compact document representation subspace that best detects the essential semantic structure. We compare the proposed LPI approach with LSI on two standard databases. Experimental results show that LPI provides better representation in the sense of semantic structure.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1993.sigirconf_conference-93.3">
          <article>
            <header>
              <h2 data-field="name">Automatic Indexing Based on Bayesian Inference Networks</h2>
            </header>

            <section data-field="body">
              <p> AbstractIn this paper, aBayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented, It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents, The indexing dictionary can be derived aut omatically from a set of manually indexed documents, An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.trec_conference-2012.60">
          <article>
            <header>
              <h2 data-field="name">Query-Structure Based Web Page Indexing</h2>
            </header>

            <section data-field="body">
              <p> Indexing is a crucial technique for dealing with the massive amount of data present on the web. In our third participation in the web track at TREC 2012, we explore the idea of building an efficient query-based indexing system over Web page collection. Our prototype explores the trends in user queries and consequently indexes texts using particular attributes available in the documents. This paper provides an in-depth description of our approach for indexing web documents efficiently; that is, topics available in the web documents are discovered with the assistance of knowledge available in Wikipedia. The welldefined articles in Wikipedia are shown to be valuable as a training set when indexing Webpages. Our complex index structure also records information from titles and urls, and pays attention to web domains. Our approach is designed to close the gaps in our approaches from the previous two years, for some queries. Our framework is able to efficiently index the 50 million pages available in the subset B of the ClueWeb09 collection. Our preliminary experiments on the TREC 2012 testing queries showed that our indexing scheme is robust and efficient for both indexing and retrieving relevant web pages, for both the ad-hoc and diversity task.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2005.sigirconf_conference-2005.2">
          <article>
            <header>
              <h2 data-field="name">Orthogonal locality preserving indexing</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTWe consider the problem of document indexing and representation. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing which is optimal in the sense of global Euclidean structure, LPI is optimal in the sense of local manifold structure. However, LPI is extremely sensitive to the number of dimensions. This makes it difficult to estimate the intrinsic dimensionality, while inaccurately estimated dimensionality would drastically degrade its performance. One reason leading to this problem is that LPI is non-orthogonal. Non-orthogonality distorts the metric structure of the document space. In this paper, we propose a new algorithm called Orthogonal LPI. Orthogonal LPI iteratively computes the mutually orthogonal basis functions which respect the local geometrical structure. Moreover, our empirical study shows that OLPI can have more locality preserving power than LPI. We compare the new algorithm to LSI and LPI. Extensive experimental results show that Orthogonal LPI obtains better performance than both LSI and LPI. More crucially, it is insensitive to the number of dimensions, which makes it an efficient data preprocessing method for text clustering, classification, retrieval, etc.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2002.ntcir_workshop-2002.12">
          <article>
            <header>
              <h2 data-field="name">Deciding Indexing Strings with Statistical Analysis</h2>
            </header>

            <section data-field="body">
              <p> Deciding indexing string is important for Information Retrieval. Ideally, the strings should be the words that represent the documents or query. Although each single word may be the first candidate of indexing strings for English corpus, it may not ideal due to the existence of compound nouns, which are often good indexing strings, and which depends on genre of corpus. The situation is even worse in Japanese or Chinese where the words are not separated by spaces. In this paper, we proposed a method to decide indexing strings based on statistical analysis. The novel features of our method are to make the most of the statistical measure called adaptation and not using language dependent resources such as dictionaries and stop words list. We have evaluated our method using Japanese test collection, and we have found that our method actually improves the precision of information retrieval systems.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2009.cikm_conference-2009.22">
          <article>
            <header>
              <h2 data-field="name">Supervised semantic indexing</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTIn this article we propose Supervised Semantic Indexing (SSI), an algorithm that is trained on (query, document) pairs of text documents to predict the quality of their match. Like Latent Semantic Indexing (LSI), our models take account of correlations between words (synonymy, polysemy). However, unlike LSI our models are trained with a supervised signal directly on the ranking task of interest, which we argue is the reason for our superior results. As the query and target texts are modeled separately, our approach is easily generalized to different retrieval tasks, such as online advertising placement. Dealing with models on all pairs of words features is computationally challenging. We propose several improvements to our basic model for addressing this issue, including low rank (but diagonal preserving) representations, and correlated feature hashing (CFH). We provide an empirical study of all these methods on retrieval tasks based on Wikipedia documents as well as an Internet advertisement task. We obtain state-of-the-art performance while providing realistically scalable methods.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.sigirconf_conference-2013.204">
          <article>
            <header>
              <h2 data-field="name">Indexing and querying overlapping structures</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTStructural information retrieval is mostly based on hierarchy. However, in real life information is not purely hierarchical and structural elements may overlap each other. The most common example is a document with two distinct structural views, where the logical view is section/ subsection/ paragraph and the physical view is page/ line. Each single structural view of this document is a hierarchy and the components are either disjoint or nested inside each other. The overlapping issue arises when one structural element cannot be neatly nested into others. For instance, when a paragraph starts in one page and terminates in the next page. Similar situations can appear in videos and other multimedia contents, where temporal or spatial constituents of a media file may overlap each other.Querying over overlapping structures is one of the challenges of large scale search engines. For instance, FSIS (FAST Search for Internet Sites) [1] is a Microsoft search platform, which encounters overlaps while analysing content of textual data. FSIS uses a pipeline process to extract structure and semantic information of documents. The pipeline contains several components, where each component writes annotations to the input data. These annotations consist of structural elements and some of them may overlap each other. Handling overlapping structures in search engines will add a novel capability of searching, where users can ask queries such as "Find all the words that overlap two lines" or "Find the music played during Intro scene of Avatar movie". There are also other use cases, where the user of the search engine is not a person, but is a specific program with complex, non-traditional information retrieval needs.This research attempts to index overlapping structures and provide efficient query processing for large-scale search engines. The current research on overlapping structures revolves around encoding and modelling data, while indexing and query processing methods need investigations. Moreover, due to intrinsic complexity of overlaps, XML indexing and query processing techniques cannot be used for overlapPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR'13, July 28-August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07. ping structures. Hence, my research on overlapping structures comprises three main parts: (1) an indexing method that supports both hierarchies and overlaps; (2) a query processing method based on the indexing technique and (3) a query language that is close to natural language and supports both full text and structural queries.Our approach for indexing overlaps is to adapt the PrePost [3] XML indexing method to overlapping structures. This method labels each node with its start and end positions and requires modest storage space. However, PrePost indexing cannot be used for overlapping nodes. To overcome this issue, we need to define a data model for overlapping structures. Since hierarchies are not sufficient to describe overlapping components, several data structures have been introduced by scholars. One of the most interesting data models is GODDAG . GODDAG is a tree-like graph, where nodes can have multiple parentage. This model can support overlaps as well as simple inheritance. Our proposed data model for indexing overlaps is such a tree-like structure, where we can define overlapping, parent-child and ancestor-descendant relationships.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2007.cikm_conference-2007.106">
          <article>
            <header>
              <h2 data-field="name">Indexing multiversion databases</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTAn efficient management of multiversion data with branched evolution is crucial for many applications. It requires database designers aware of tradeoffs among index structures and policies. This paper defines a framework and an analysis method for understanding the behavior of different indexing policies. Given data and query characteristics the analysis allows determining the most suitable index structure. The analysis is validated by an experimental study.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.cikm_conference-2012.137">
          <article>
            <header>
              <h2 data-field="name">Robust distributed indexing for locality-skewed workloads</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTMultidimensional indexing is crucial for enabling a fast search over large-scale data. Owing to the unprecedented scale of data, extending such indexing technology has recently gained attention in distributed environments. The goal of existing efforts in distributed indexing has been the localization of queries to data residing at a small number of nodes (i.e., locality-preserving indexing) to minimize communication cost. However, considering that workloads often correlate with data locality, such indexing often generates hotspots. Location-based queries are typically skewed to disaster areas during certain periods of time, e.g., during Hurricane Irene, search traffic increased by more than 2000%. To alleviate such hotspots, we propose workload-balancing as an optimization goal. A cost model analytically supporting the need for load balancing is first developed, then a distributed index that evenly distributes the workload is presented. Our empirical study suggests that hotspots degrading search performance can be effectively alleviated. Specifically, when deployed to Amazon EC2, our proposed scheme showed maximum speed-up of 127.7%. Even in hostile settings where workload is not at all correlated with the search criteria, the proposed scheme's performance is comparable to existing approaches optimized for such settings.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2005.sigirconf_conference-2005.35">
          <article>
            <header>
              <h2 data-field="name">Multi-label informed latent semantic indexing</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTLatent semantic indexing (LSI) is a well-known unsupervised approach for dimensionality reduction in information retrieval. However if the output information (i.e. category labels) is available, it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set. This is of particular importance in applications with multiple labels, in which each document can belong to several categories simultaneously. In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs. The recovered "latent semantics" thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy. Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.cikm_workshop-2013dare.2">
          <article>
            <header>
              <h2 data-field="name">Indexing electronic medical records using a taxonomy</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTWith the move toward patient electronic medical records (EMRs), accessing information for insurance coding and research depends on standardized taxonomies to organize and index the content. Controlled vocabularies are necessary to interpret content consistently. Established quasi-taxonomies provide codes for medical conditions and treatments, but applying these codes as metadata to index the records is laborious, requiring translation from natural language in the EMR to a code's verbal equivalent to the code itself. Indexing systems can streamline the categorization process for greater efficiency and accuracy by using Bayesian engines or a rule-based approach. Analyzing discrepancies between human indexing and the software system results shows where editorial intervention is needed for continual improvement, with a goal of 85% or higher accuracy. Using a categorization system with a hierarchical taxonomy enables deep, precise indexing or quick, automatic filtering to more general concepts. The accuracy of medical indexing systems varies widely, based on the degree of automation and capacity for semantic analysis.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2001.ntcir_workshop-2001.15">
          <article>
            <header>
              <h2 data-field="name">Hybrid Term Indexing: an Evaluation</h2>
            </header>

            <section data-field="body">
              <p> Retrieval effectiveness depends on how terms are extracted and indexed. For Chinese text (and others like Japanese and Korean), there are no space to delimit words. Indexing using hybrid terms</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2002.ntcir_workshop-2002.14">
          <article>
            <header>
              <h2 data-field="name">Different Retrieval Models and Hybrid Term Indexing</h2>
            </header>

            <section data-field="body">
              <p> Retrieval effectiveness depends on both the retrieval model and how terms are extracted and indexed. For Chinese, Japanese and Korea text, there are no spaces to delimit words. Indexing using hybrid terms (i.e. words and bigrams) was not very effective in NTCIR-II open evaluation. In this evaluation, we found that using the 2-Poisson model with hybrid term indexing can be effective in retrieval. With our pseudo-relevance feedback, the performance can be enhanced to a level that is comparable to the best performance in the formal runs. Therefore, we found that hybrid term indexing is promising when the 2-Poisson model is used.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2006.ipm_journal-ir0anthology0volumeA42A1.7">
          <article>
            <header>
              <h2 data-field="name">Concept integration of document databases using different indexing languages</h2>
            </header>

            <section data-field="body">
              <p> AbstractAn integrated information retrieval system generally contains multiple databases that are inconsistent in terms of their content and indexing. This paper proposes a rough set-based transfer (RST) model for integration of the concepts of document databases using various indexing languages, so that users can search through the multiple databases using any of the current indexing languages. The RST model aims to effectively create meaningful transfer relations between the terms of two indexing languages, provided a number of documents are indexed with them in parallel. In our experiment, the indexing concepts of two databases respectively using the Thesaurus of Social Science (IZ) and the Schlagwortnormdatei (SWD) are integrated by means of the RST model. Finally, this paper compares the results achieved with a cross-concordance method, a conditional probability based method and the RST model.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="1995.sigirconf_conference-95.17">
          <article>
            <header>
              <h2 data-field="name">Evaluation of Evaluation in Information Retrieval</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTEvaluation is a major force in research, development and applications related to information retrieval (IR). This paper is a critical and historical analysis of evaluations of IR systems and processes. Strengths and shortcomings of evaluation efforts and approaches are discussed. together with major challenges and questions. A limited comparison is made with evaluation in experts systems and Online Public Access Catalogs (OPACS). Evaluation is further analyzed in relation to the broad context and specific problems addressed. Levels of evaluation are identified and contrasted. most IR evaluations were concerned with the processing level, but others were conducted at the output, users and use, and social levels. A major problem is the isolation of evaluations at a given level. Issues related to systems under evaluation, and evaluation criteria, measures, measuring instruments, and methodologies are examined. A general point is also considered: IR is mcreasmgly imbedded into many other applications, such as the Internet or digital libraries.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2010.sigirconf_conference-2010.177">
          <article>
            <header>
              <h2 data-field="name">Retrieval system evaluation: automatic evaluation versus incomplete judgments</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTIn information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2005.sigirconf_conference-2005.52">
          <article>
            <header>
              <h2 data-field="name">Evaluation of resources for question answering evaluation</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTControlled and reproducible laboratory experiments, enabled by reusable test collections, represent a well-established methodology in modern information retrieval research. In order to confidently draw conclusions about the performance of different retrieval methods using test collections, their reliability and trustworthiness must first be established. Although such studies have been performed for ad hoc test collections, currently available resources for evaluating question answering systems have not been similarly analyzed. This study evaluates the quality of answer patterns and lists of relevant documents currently employed in automatic question answering evaluation, and concludes that they are not suitable for post-hoc experimentation. These resources, created from runs submitted by TREC QA track participants, do not produce fair and reliable assessments of systems that did not participate in the original evaluations. Potential solutions for addressing this evaluation gap and their shortcomings are discussed.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2018.ecir_conference-2018.43">
          <article>
            <header>
              <h2 data-field="name">A Meta-Evaluation of Evaluation Methods for Diversified Search</h2>
            </header>

            <section data-field="body">
              <p> Abstract. For the evaluation of diversified search results, a number of different methods have been proposed in the literature. Prior to making use of such evaluation methods, it is important to have a good understanding of how diversity and relevance contribute to the performance metric of each method. In this paper, we use the statistical technique ANOVA to analyse and compare three representative evaluation methods for diversified search, namely α-nDCG, MAP-IA, and ERR-IA, on the TREC-2009 Web track dataset. It is shown that the performance scores provided by those evaluation methods can indeed reflect two crucial aspects of diversity -richness and evenness -as well as relevance, though to different degrees.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.sigirjournals_journal-ir0anthology0volumeA42A2.0">
          <article>
            <header>
              <h2 data-field="name">Crowdsourcing for relevance evaluation</h2>
            </header>

            <section data-field="body">
              <p> AbstractRelevance evaluation is an essential part of the development and maintenance of information retrieval systems. Yet traditional evaluation approaches have several limitations; in particular, conducting new editorial evaluations of a search system can be very expensive. We describe a new approach to evaluation called TERC, based on the crowdsourcing paradigm, in which many online users, drawn from a large community, each performs a small evaluation task.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.ecir_conference-2013.107">
          <article>
            <header>
              <h2 data-field="name">Practical Online Retrieval Evaluation</h2>
            </header>

            <section data-field="body">
              <p>MotivationGiven the growing breadth of IR research, the need to accurately evaluate IR systems has likewise been gaining in importance. One particularly important research question, and the one that we address in this tutorial, is how to design and employ online, in-situ, evaluation techniques to reliably and efficiently evaluate the impact of retrieval techniques on real users. Our focus will be on how to do this without necessitating access to commercial scale search traffic.To make a case for practical online retrieval evaluation, online techniques must be compared to more common approaches -in particular to manually assessing performance on a sample of queries, and a fixed document collection, with a selection of the documents being judged for relevance to the queries. Such judgments are often difficult and expensive to acquire , and researchers outside of the major search companies largely rely on collections produced by evaluation campaigns, such as TREC [24]  and CLEF [7].</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2017.sigirconf_conference-2017.4">
          <article>
            <header>
              <h2 data-field="name">Meta-evaluation of Online and Offline Web Search Evaluation Metrics</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTAs in most information retrieval (IR) studies, evaluation plays an essential part in Web search research. Both o ine and online evaluation metrics are adopted in measuring the performance of search engines. O ine metrics are usually based on relevance judgments of query-document pairs from assessors while online metrics exploit the user behavior data, such as clicks, collected from search engines to compare search algorithms. Although both types of IR evaluation metrics have achieved success, to what extent can they predict user satisfaction still remains under-investigated. To shed light on this research question, we meta-evaluate a series of existing online and o ine metrics to study how well they infer actual search user satisfaction in di erent search scenarios. We nd that both types of evaluation metrics signi cantly correlate with user satisfaction while they re ect satisfaction from di erent perspectives for di erent search tasks. O ine metrics be er align with user satisfaction in homogeneous search (i.e. ten blue links) whereas online metrics outperform when vertical results are federated. Finally, we also propose to incorporate mouse hover information into existing online evaluation metrics, and empirically show that they be er align with search user satisfaction than click-based online metrics.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2015.wwwconf_conference-2015c.214">
          <article>
            <header>
              <h2 data-field="name">Online Search Evaluation with Interleaving</h2>
            </header>

            <section data-field="body">
              <p> AbstractOnline evaluation allows information retrieval systems to be assessed based on how real users respond to search results presented. Compared with traditional offline evaluation based on manual relevance assessments, online evaluation is particularly attractive in settings where reliable assessments are difficult or too expensive to obtain.However, the successful use of online evaluation requires the right metrics to be used, as real user behaviour is often difficult to interpret. I will present interleaving, a sensitive online evaluation approach that creates paired comparisons for every user query, and compare it with alternative A/B online evaluation approaches. I will also show how interleaving can be parameterized to create a family of evaluation metrics that can be chosen to best match the goals of an evaluation.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2011.sigirconf_conference-2011.220">
          <article>
            <header>
              <h2 data-field="name">Practical online retrieval evaluation</h2>
            </header>

            <section data-field="body">
              <p>1301</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2017.cikm_conference-2017.74">
          <article>
            <header>
              <h2 data-field="name">Scaling Probabilistic Temporal Query Evaluation</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTOpen information extraction has driven automatic construction of (temporal) knowledge graphs (e.g. YAGO) that maintain probabilistic (temporal) facts and inference rules. One of the most important tasks in these knowledge graphs is query evaluation. This task is well known to be #P-hard. One of the bottlenecks of probabilistic (temporal) query evaluation is finding efficient ways of grounding the query and inference rules, to generate a factor graph that can be used for approximate query evaluation or to retrieve lineages of queries for exact evaluation. In this work, we propose the PRATiQUE (PRobAbilistic Temporal QUery Evaluation) framework for scalable temporal query evaluation. It harnesses the structure of temporal inference rules for efficient in-database grounding, i.e., it uses partitions to store structurally equivalent rules. Besides, PRATiQUE leverages a state-of-the-art Gibbs sampler to compute marginal probabilities of query answers. We report on an extensive experimental evaluation, which confirms the efficiency of our proposal.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2011.cikm_conference-2011.165">
          <article>
            <header>
              <h2 data-field="name">External evaluation measures for subspace clustering</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTKnowledge discovery in databases requires not only development of novel mining techniques but also fair and comparable quality assessment based on objective evaluation measures. Especially in young research areas where no common measures are available, researchers are unable to provide a fair evaluation. Typically, publications glorify the high quality of one approach only justified by an arbitrary evaluation measure. However, such conclusions can only be drawn if the evaluation measures themselves are fully understood.In this paper, we provide the basis for systematic evaluation in the emerging research area of subspace clustering. We formalize general quality criteria for subspace clustering measures not yet addressed in the literature. We compare the existing external evaluation methods based on these criteria and pinpoint limitations. We propose a novel external evaluation measure which meets the requirements in form of quality properties. In thorough experiments we empirically show characteristic properties of evaluation measures. Overall, we provide a set of evaluation measures that fulfill the general quality criteria as recommendation for future evaluations. All measures and datasets are provided on our website 1 and are integrated in our evaluation framework.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2000.sigirconf_conference-2000.6">
          <article>
            <header>
              <h2 data-field="name">Evaluating evaluation measure stability</h2>
            </header>

            <section data-field="body">
              <p> AbstractThis paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2008.sigirconf_conference-2008.89">
          <article>
            <header>
              <h2 data-field="name">Evaluation measures for preference judgments</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTThere has been recent interest in collecting user or assessor preferences, rather than absolute judgments of relevance, for the evaluation or learning of ranking algorithms. Since measures like precision, recall, and DCG are defined over absolute judgments, evaluation over preferences will require new evaluation measures that explicitly model them. We describe a class of such measures and compare absolute and preference measures over a large TREC collection.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2019.ecir_conference-20192.36">
          <article>
            <header>
              <h2 data-field="name">CLEF eHealth 2019 Evaluation Lab</h2>
            </header>

            <section data-field="body">
              <p> Abstract. Since 2012 CLEF eHealth has focused on evaluation resource building efforts around the easing and support of patients, their next-ofkins, clinical staff, and health scientists in understanding, accessing, and authoring eHealth information in a multilingual setting. This year's lab offers three tasks: Task 1 on multilingual information extraction; Task 2 on technology assisted reviews in empirical medicine; and Task 3 on consumer health search in mono-and multilingual settings. Herein, we describe the CLEF eHealth evaluation series to-date and then present the 2019 tasks, evaluation methodology, and resources.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.sigirjournals_journal-ir0anthology0volumeA46A1.10">
          <article>
            <header>
              <h2 data-field="name">Theoretical evaluation of XML retrieval</h2>
            </header>

            <section data-field="body">
              <p> AbstractThis thesis has developed a theoretical framework to evaluate XML retrieval. XML retrieval deals with retrieving those document parts that specifically answer a query. It is concerned with using the document structure to improve the retrieval of information from documents by only delivering those parts of a document an information need is about. We define a theoretical evaluation methodology based on the idea of 'aboutness' and apply it to XML retrieval models.Situation Theory is used to express the aboutness proprieties of XML retrieval models. We propose a Situation Theory framework to evaluate XML retrieval, which is based on the basic and most general information retrieval question how a document (or in our case an XML element) can be about a query. This framework allows us to compare and analyze the reasoning behaviour of XML retrieval models experimented within INEX evaluation campaigns. We develop a dedicated methodology for the evaluation of XML retrieval and apply this methodology to five XML retrieval models from INEX. For each model we derive functional and qualitative properties that qualify its formal behaviour. We compare this behaviour with the underlying flat document retrieval model as well as with a model we specially design to determine how much an XML retrieval model includes XML structure in its reasoning behaviour.More INEX specific, this thesis further investigates the use of our theoretical evaluation methodology to describe the INEX evaluation methodology. We exemplify theoretical models of user agents and assessment procedures in INEX and derive reasoning assumptions that are included in the specific XML retrieval experimental evaluation, its scales and the ways assessments are done. We point to potential inconsistencies and make suggestions for alternative views on the experimental evaluation dimensions for XML retrieval. Further INEX specifics are discussed when we theoretically analyse filters, as they are used in INEX to deliver only specific answers to an information need. We introduce our theoretical methodology to analyse filters as special aboutness decisions, before applying it to the XML retrieval filtering models.We finally use the theoretical properties of XML retrieval models and their filters to explain experimental results obtained with some of the XML retrieval models within INEX and draw upon all our previous results to demonstrate how theoretical evaluation insights can</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2012.tist_journal-ir0anthology0volumeA3A4.16">
          <article>
            <header>
              <h2 data-field="name">Evaluation of Folksonomy Induction Algorithms</h2>
            </header>

            <section data-field="body">
              <p> Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2001.clef_workshop-2001.34">
          <article>
            <header>
              <h2 data-field="name">The Philosophy of Information Retrieval Evaluation</h2>
            </header>

            <section data-field="body">
              <p> Abstract. Evaluation conferences such as TREC, CLEF, and NTCIR are modern examples of the Cranfield evaluation paradigm. In Cranfield, researchers perform experiments on test collections to compare the relative effectiveness of different retrieval approaches. The test collections allow the researchers to control the effects of different system parameters, increasing the power and decreasing the cost of retrieval experiments as compared to user-based evaluations. This paper reviews the fundamental assumptions and appropriate uses of the Cranfield paradigm, especially as they apply in the context of the evaluation conferences.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2017.ictir_conference-2017.27">
          <article>
            <header>
              <h2 data-field="name">Information Retrieval Evaluation as Search Simulation: A General Formal Framework for IR Evaluation</h2>
            </header>

            <section data-field="body">
              <p>CCS CONCEPTS• Information systems → Evaluation of retrieval results;</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2013.sigirjournals_journal-ir0anthology0volumeA47A2.0">
          <article>
            <header>
              <h2 data-field="name">Evaluation as a service for information retrieval</h2>
            </header>

            <section data-field="body">
              <p> AbstractHow can we run large-scale, community-wide evaluations of information retrieval systems if we lack the ability to distribute the document collection on which the task is based? This was the challenge we faced in the TREC Microblog tracks over the past few years. In this paper, we present a novel evaluation methodology we dub "evaluation as a service", which was implemented at TREC 2013 to address restrictions on data redistribution. The basic idea is that instead of distributing the document collection, we (the track organizers) provided a service API "in the cloud" with which participants could accomplish the evaluation task. We outline advantages as well as disadvantages of this evaluation methodology, and discuss how the approach might be extended to other evaluation scenarios.</p>
            </section>
          </article>
        </li>
      
        <li data-bio-id="2006.sigirconf_conference-2006.36">
          <article>
            <header>
              <h2 data-field="name">Minimal test collections for retrieval evaluation</h2>
            </header>

            <section data-field="body">
              <p> ABSTRACTAccurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of realworld implementations is at best inefficient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95% confidence.</p>
            </section>
          </article>
        </li>
      
    </ol>
   </div>
  <script src="index.js"></script>
</body>
